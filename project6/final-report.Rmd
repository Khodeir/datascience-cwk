---
title: "Final Report"
author: "Mohamed Khodeir"
date: "December 8, 2015"
output: html_document
---
## Experiment Design

It is important to keep in mind the experiment hypothesis when considering design choices. So I will include it here:

“The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time without significantly reducing the number of students to continue past the free trial and eventually complete the course.”

## Metric Choice


**Number of cookies**

The total unique pageviews was important as an invariant metric firstly because it serves as a good sanity check of the group assigning mechanism. 
It should be completely unaffected by the proposed change, so it is not suitable as an evaluation metric.

**Number of user-ids**

The number of users to enroll should be reduced, assuming the number of clicks stays the same in the experiment. Therefore, this can’t be used as an invariant metric.
This suggests that it can be used as an evaluation metric, provided the number of clicks is also tracked as an invariant metric. However, opt instead to use gross conversion since - as a probability - it is more amenable to analysis.

**Number of clicks**

The number of clicks to the start free trial button is meant to be unchanged since the proposed feature only comes into effect after the user has already clicked. This, and the fact that it is the unit of analysis for both gross and net conversion make it an ideal and necessary invariant metric. 

**Click-through-probability**

The click-through-probability also should remain unchanged for the same reasons (as number of clicks and cookies), but because it is simply a ratio of clicks to total cookies and these are both used as invariant metrics, it would be redundant to include.

**Gross conversion**

Gross conversion concerns the first part of the stated experiment's hypothesis. By directing those who indicate less than 5 hour weekly commitment to access the course material for free, the proposed feature should reduce the number of students who enroll. This makes it a necessary evaluation metric. 

**Net conversion**

Net conversion concerns the second part of our hypothesis. Namely, the number of students to continue past the free trial is meant to be reduced insignificantly (or remain unchanged.)
This is the truly crucial metric for our purposes, so it is quite necessary as an evaluation metric.

**Retention**

Retention can be used as an evaluation metric. We would expect its denominator  - the number of user-ids - to decrease and its numerator - the number of users to remain enrolled past the 14-day boundary -  to be relatively unchanged. 
However, this is not a necessary metric as its purposes are served by the combination of gross and net conversion which form a more complete and easily interpretable evaluation.

**Summary**

The invariant metrics I am using are the Number of Cookies (unique pageviews) and the Number of Clicks (to the free trial button). 

The evaluation metrics are Gross and Net Conversion. A successful outcome for the experiment is that Gross Conversion is significantly decreased **and** Net conversion does not (significantly decrease).
 
## Measuring Standard Deviation


| Metric           | Analytic Standard Deviation |
|------------------|-----------------------------|
| Gross Conversion | 0.02023                     |
| Net Conversion   | 0.01560                     |

I expect both estimates for gross and net conversion to be fairly faithful to the empirical estimate. For one thing, they both have the same unit of analysis as the unit of diversion - the cookie. Also, I have no reason to suspect the baseline values of being inaccurate. 

## Sizing

### Number of Samples vs. Power

I am not using the Bonferroni correction for multiple comparisons. The feature being tested will only be considered successful if both the gross conversion is significantly reduced and the net conversion is relatively unchanged. Therefore, I do not need to correct for the increased likelihood of either one of my comparisons being erroneously significant.

The number of pageviews I will need is thus: 679,300

### Duration vs. Exposure

I chose to direct 100% of the traffic to the experiment. Under normal circumstances for Udacity, the experiment should take little less than 17 days to complete.

I have incomplete information for this decision, because I do not know the number of other experiments that need to run, and the proportions of the user base they would need in order to complete in a reasonable duration. However, the change does not seem risky since it only slightly perturbs the a single use case, and so we should want to complete the experiment in the minimum possible duration. 

There is a slightly subtle point here. Since we need at least 14 days to pass between a user enrolling, and Udacity checking the corresponding payments, we would not be in a position to analyze the experiment results until 14 days after the end of the experiment. Therefore the total duration of the experiment proper would be 31 days. (And other experiments that may affect the evaluation metrics should not be allowed to run in those last 14 days.)

## Experiment Analysis

###Sanity Checks


| Metric            | Confidence Interval (95%) | Observed Value | Passes Sanity Check? |
|-------------------|---------------------------|----------------|----------------------|
| Number of cookies | [0.4988, 0.5012]           | 0.5006         | Yes                  |
| Number of clicks  | [0.4958, 0.5042]           | 0.5005         | Yes                  |

### Result Analysis
**Effect Size Tests**

| Metric           | Confidence Interval (95%) | Statistically Significant? | Practically Significant? |
|------------------|---------------------------|----------------------------|--------------------------|
| Gross Conversion | [-0.0291, -0.012]         | Yes                        | Yes                      |
| Net Conversion   | [-0.0116, 0.0019]         | No                         | No                       |

**Sign Tests**

| Metric           | P-value | Statistically Significant ($\alpha=0.05$) |
|------------------|---------|-------------------------------------------|
| Gross Conversion | 0.0026  | Yes                                       |
| Net Conversion   | 0.6776  | No                                        |



**Summary**

Again, I am not using the Bonferroni correction for multiple comparisons. The feature being tested will only be considered successful if both the gross conversion is significantly reduced and the net conversion is not. Therefore, I do not need to correct for the increased likelihood of either one of my comparisons being erroneously significant.

## Recommendation

My recommendation for Udacity is that the feature in question not be implemented immediately. 

The tests have shown that it is very likely (95%) that the feature will reduce the number of students enrolling for the course. If this effect is combined with an unaffected net conversion rate, then the feature will have reduced Udacity's cost (Coaching Time) per paying student.

And although we found the change in net conversion to be statistically and practically insignificant, our confidence interval for its effect size does include the negative practical significance boundary (-0.0075). This means that we cannot say confidently that the net conversion did not decrease by less than .75%.

This makes implementing the feature unnecessarily risky, and we should either devote more resources to testing the effect (and reduce the size of the confidence interval), or abandon the feature.

## Follow-Up Experiment

I would be interested in running an experiment to test the effect of email status/progress reports on student retention (past the 14 day boundary). My conjecture is that a student’s motivation and commitment to their online courses may decrease over time. This can first be checked against observational data from Udacity’s records.

If it is well supported by observational data, I would hypothesize that a periodic recognition of accomplishments in the course, as well as time spent on online materials (that Udacity can track), could have a positive effect on a student’s level of involvement in the course. This would then translate into higher retention.

To test this, we have to first decide on a reasonable interval between progress report emails.

We could perform a multivariate test by splitting subjects into a number of groups each having a email frequency, as well as a control group. However, this will necessarily take a longer time, and it would be unwise given that this feature may somehow have the exact opposite of the hypothesized effect. 

We want to affect the retention of students in a two week period, so I would suggest a 3-day interval between progress reports.

If we choose the unit of diversion to be the user-id, and directly evaluate retention rates, then this experiment may take too long. However, weekly emails are not a very risky feature, and we can allow for the user to opt-out, in which case a second metric would be opt-out rates.

To reduce the runtime of experiment, we can use net conversion as our evaluation metric - as we did in this experiment. One caveat, however, is that the unit of analysis in that case would be the number of clicks to start a free trial, and so our analytic estimate of variance may first need to be revised empirically in order to accurately size the experiment.

